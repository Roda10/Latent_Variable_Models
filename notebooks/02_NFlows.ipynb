{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roda10/Latent_Variable_Models/blob/main/notebooks/02_NFlows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRAqr4d2_59P"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/02_NFlows.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcz-MDZ-_59T"
      },
      "source": [
        "## Leveraging normalizing flows for SBI\n",
        "**Pranav Mamidanna, PhD** (p.mamidanna22@imperial.ac.uk), April 2025\n",
        "\n",
        "In the previous tutorial, we have seen how we can use ABC to estimate the posterior distribution of the parameters of a simulator. However, ABC is known to be inefficient in high dimensions, and depends on the choice of several hyperparameters.\n",
        "\n",
        "In this notebook:\n",
        "1. We will see how we can use normalizing flows to estimate the posterior distribution of the parameters of a simulator.\n",
        "2. We will train an affine coupling layer from scratch, and use it to build a normalizing flow.\n",
        "3. We will use the normalizing flow to directly estimate the \"two moons\" posterior, the so-called \"neural posterior estimation\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Tzie--_59V"
      },
      "outputs": [],
      "source": [
        "# NOTE: Takes ~3 min to run\n",
        "!python -m pip install sbi corner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKC62Z6W_59W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import clear_output, display\n",
        "from ipywidgets import interact, FloatSlider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC07FZYQ_59W"
      },
      "outputs": [],
      "source": [
        "# Recall the two moons model\n",
        "def two_moons_sbi(theta, sigma=0.01):\n",
        "    \"\"\"Generate a two moons posterior\"\"\"\n",
        "    batch_size = theta.shape[0]\n",
        "    alpha = torch.rand(batch_size) * torch.pi - torch.pi/2  # Uniform(-pi/2, pi/2)\n",
        "    r = sigma * torch.randn(batch_size) + 1  # Normal(1, sigma)\n",
        "\n",
        "    x_1 = r * torch.cos(alpha) + 0.5 - torch.abs(theta[:, 0] + theta[:, 1])/torch.sqrt(torch.tensor(2))\n",
        "    x_2 = r * torch.sin(alpha) + (- theta[:, 0] + theta[:, 1])/torch.sqrt(torch.tensor(2))\n",
        "\n",
        "    x =  torch.stack([x_1, x_2], dim=-1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWdhTmJm_59X"
      },
      "source": [
        "### **What are normalizing flows?**  \n",
        "Normalizing flows are a class of generative models that can learn and represent complex probability distributions. They do this by starting with a simple “base” distribution (e.g., a standard normal) and applying a series of invertible transformations to map samples from that base distribution into the target (or data) distribution. Importantly, because these transformations are invertible, we can also evaluate the probability density of any sample by mapping it back to the base distribution!\n",
        "\n",
        "Let's see how that works below. Let's first demonstrate what normalizing flows do by manipulating what happens to the distribution of $x$, under a given base density and $f$. Here, we will use the tanh function to transform the base density.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moolbNPY_59Y"
      },
      "outputs": [],
      "source": [
        "def NF_intuition_plot(scale=1.0):\n",
        "    # Get samples from base density\n",
        "    n_samples = 5000\n",
        "    np.random.seed(129)\n",
        "    base_samples = np.random.randn(n_samples) / 2  # Standard normal\n",
        "\n",
        "    # Define the transform using tanh\n",
        "    transform = lambda z: np.tanh(scale * z)\n",
        "\n",
        "    # Generate a set of x, y values for plotting\n",
        "    x_vals = np.linspace(-3, 3, 200)\n",
        "    y_vals = transform(x_vals)\n",
        "\n",
        "    # Transform the base samples\n",
        "    transformed_samples = transform(base_samples)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "    # Plot base density\n",
        "    sns.kdeplot(base_samples, ax=axes[0], fill=True, color='C0')\n",
        "    axes[0].plot(base_samples, np.full_like(base_samples, -0.02), '|', color='C0')\n",
        "    axes[0].set_title('Base Density')\n",
        "    axes[0].set_ylim([-0.05, None]); axes[0].set_xlim([-2, 2])\n",
        "\n",
        "    # Plot transformation function\n",
        "    axes[1].plot(x_vals, y_vals, label='tanh transform')\n",
        "    axes[1].set_xlim([-2, 2]); axes[1].set_ylim([-1, 1])\n",
        "    axes[1].legend(); axes[1].grid(True)\n",
        "    axes[1].set_title(f'Transform (scale={scale})')\n",
        "\n",
        "    # Plot transformed density\n",
        "    sns.kdeplot(transformed_samples, ax=axes[2], fill=True, color='C1')\n",
        "    axes[2].plot(transformed_samples, np.full_like(transformed_samples, -0.02), '|', color='C1')\n",
        "    axes[2].set_title('Transformed Density')\n",
        "    axes[2].set_ylim([-0.05, None]); axes[2].set_xlim([-2, 2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create an interactive widget that lets you control the scale parameter\n",
        "scale_slider = FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='Scale:')\n",
        "interact(NF_intuition_plot, scale=scale_slider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm7qy7zc_59Z"
      },
      "source": [
        "Its very easy to see how to draw samples. But, how do we get the probabilities?\n",
        "\n",
        "**Change of variables formula**  \n",
        "The magic basically happens within the simple \"change of variables\" formula. It tells us how densities transform under invertible mappings.\n",
        "\n",
        "Let $z$ be a sample from a known \"base\" density, like a normal distribution, whose probability density is given by $p_Z(z)$. Then, $x = f(z)$ the corresponding transformed sample has a probability density given as $$p_X(x) = p_Z(z)\\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right|$$ That is, if we have the right $f$ that maps $x$ to $z$, we can estimate the probability density of any given $x$ under the learned distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65qM-xSg_59a"
      },
      "source": [
        "> **Task 2.1** Implement the change of variables formula to estimate the probability density of $x$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psCDwdW9_59b"
      },
      "outputs": [],
      "source": [
        "def NF_intuition_plot(scale=1.0):\n",
        "    n_samples = 5000\n",
        "    np.random.seed(129)\n",
        "    base_samples = np.random.randn(n_samples) / 2.0   # σ = 0.5\n",
        "\n",
        "    ## TODO: Implement the following lambda functions\n",
        "    # transforms\n",
        "    transform = lambda z: np.tanh(scale * z)\n",
        "    inverse = lambda x:\n",
        "    inv_jac = lambda x:\n",
        "\n",
        "    # densities\n",
        "    p_z = lambda z:\n",
        "    p_x = lambda x:\n",
        "\n",
        "    # evaluate on grid for smooth curve\n",
        "    x_grid = np.linspace(-2, 2, n_samples)\n",
        "    px_grid = p_x(x_grid)\n",
        "\n",
        "    # transform samples\n",
        "    transformed_samples = transform(base_samples)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "    sns.kdeplot(base_samples, ax=axes[0], fill=True, color='C0')\n",
        "    axes[0].plot(base_samples, np.full_like(base_samples, -0.02), '|', color='C0')\n",
        "    axes[0].set_title('Base Density'); axes[0].set_ylim([-0.05, None]); axes[0].set_xlim([-2, 2])\n",
        "\n",
        "    xx = np.linspace(-2, 2, n_samples)\n",
        "    axes[1].plot(xx, transform(xx), label='tanh'); axes[1].grid(True)\n",
        "    axes[1].set_title(f'Transform (scale={scale})'); axes[1].set_xlim([-2, 2]); axes[1].set_ylim([-1.1, 1.1])\n",
        "\n",
        "    sns.kdeplot(transformed_samples, ax=axes[2], fill=True, color='C1', label='KDE')\n",
        "    axes[2].plot(x_grid, px_grid, 'r', lw=2, label='Analytical')\n",
        "    axes[2].set_title('Transformed density'); axes[2].set_xlim([-2, 2]); axes[2].set_ylim([-0.05, None])\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Create an interactive widget that lets you control the scale parameter\n",
        "scale_slider = FloatSlider(value=1.0, min=0.5, max=2.0, step=0.1, description='Scale:')\n",
        "interact(NF_intuition_plot, scale=scale_slider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEYjPpz__59c"
      },
      "source": [
        "#### **The API**\n",
        "\n",
        "So far, we have seen how a bijective function can be used to transform a given \"base\" density. The real strength of normalizing flows comes from the fact that we can basically do the reverse - we can start with an unknown data density and learn the bijective function that transforms it into a known base density!\n",
        "\n",
        "There are two components to a normalizing flow: (1) the bijective transform (or a set of them), and (2) the prior distribution. Once we have these, at train time, we can compute $z$ given a batch of ($x$, context), and evaluate the loss. At test time, we can draw a sample from ($z$, context), and generate samples from the learned data distribution $x$ (and evaluate the probability density!!).\n",
        "\n",
        "To do this, we will need:\n",
        "\n",
        "`NF = NormalizingFlow(flows, prior)`\n",
        "- `NF.forward(x, context) -> z, ldj`,\n",
        "- `NF.sample(z, context) -> x, ldj`\n",
        "\n",
        "The `flows` object is usually a list of layers each of which is a bijective transform:\n",
        "\n",
        "`T = FlowTransform()`\n",
        "- `T.forward(x, context) -> z, ldj`\n",
        "- `T.inverse(z, context) -> x, ldj`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj97rGaq_59c"
      },
      "source": [
        "#### **Why are they suitable for Simulation-Based Inference?**\n",
        "\n",
        "Here, we will specifically deal with the case of Neural Posterior Estimation.\n",
        "\n",
        "In the previous tutorial, we have seen that SBI basically deals with estimating the posterior distribution, i.e., estimating the density $p(\\theta \\mid x_{\\text{obs}})$. One way to do this using normalizing flows, is by sampling from a base density (priors on the parameters of interest), and learning how to transform these directly into posterior samples! This is the idea behind neural posterior estimation.\n",
        "\n",
        "With a simulator in hand, both these steps become possible! Sample from a prior distribution over $\\theta$, plug it into the simulator to obtain $x$. You train a normalizing flow to learn how $p(\\theta)$ is transformed to $p(\\theta \\mid x)$!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS7ipFq3_59d"
      },
      "source": [
        "### **Real NVP - Affine Coupling Flows**\n",
        "\n",
        "A popular implementation of normalizing flows is **Real NVP** (Dinh et al., 2017). The core of Real NVP is the **affine coupling** transformation.\n",
        "\n",
        "Suppose we split the data $x$ of dimension $D$ into two parts, $x = [x_{1:d},\\, x_{(d+1):D}]$. In a **coupling layer**, we leave one part unchanged and apply a learnable affine transformation to the other part. Specifically, let:\n",
        "\n",
        "$$\n",
        "    y_{1:d} \\;=\\; x_{1:d},\n",
        "$$\n",
        "$$\n",
        "    y_{(d+1):D} \\;=\\; x_{(d+1):D}\\,\\odot \\exp\\bigl(s_\\theta(x_{1:d})\\bigr)\\;+\\; t_\\theta(x_{1:d}),\n",
        "$$\n",
        "\n",
        "where $\\odot$ denotes elementwise multiplication. The functions $s_\\theta(\\cdot)$ and $t_\\theta(\\cdot)$ (the \"scale\" and \"shift\" networks) are typically small neural networks that depend on the \"frozen\" part $x_{1:d}$.\n",
        "\n",
        "- **Invertibility**: This transformation is **invertible** because you can solve for $x_{(d+1):D}$ by reversing the shift and scale operations:\n",
        "  $$\n",
        "    x_{(d+1):D}\n",
        "     = \\Bigl(y_{(d+1):D} - t_\\theta(y_{1:d})\\Bigr)\\,\\odot \\exp\\Bigl(- s_\\theta(y_{1:d})\\Bigr).\n",
        "  $$\n",
        "  The log-determinant of the Jacobian $\\left\\lvert \\det \\frac{\\partial y}{\\partial x} \\right\\rvert$ is simply\n",
        "  $$\n",
        "    \\sum_{j=1}^{D-d} s_\\theta(x_{1:d})_j,\n",
        "  $$\n",
        "  because the scaling is diagonal in the sub-block.\n",
        "\n",
        "- **Why is it non-linear?**\n",
        "  Although the transformation is written as an *affine* function for the second block, the parameters of that affine transformation are themselves neural-network outputs, i.e., $s_\\theta(\\cdot)$ and $t_\\theta(\\cdot)$. This makes the overall mapping\n",
        "  $$\n",
        "    x \\mapsto y\n",
        "  $$\n",
        "  non-linear in $x$. The \"frozen\" part $x_{1:d}$ is feeding through a neural network to produce scale and shift factors, which can be highly non-linear functions of $x_{1:d}$.\n",
        "\n",
        "- **Coupling layers and permutations**: In Real NVP, we often interleave such coupling layers with permutation layers to ensure that over multiple layers, each dimension eventually appears in the \"frozen\" part and the \"transformed\" part. This broadens the flexibility of the flow, letting it model complex dependencies across all dimensions.\n",
        "\n",
        "In summary, Real NVP is a straightforward yet powerful example of how normalizing flows combine tractable Jacobians (via affine coupling) with flexible function approximators (neural networks for scale and shift).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv1HW5zx_59d"
      },
      "source": [
        "> **Task 2.2.** We have seen above that for NPE, we need to train a conditional normalizing flow, where $x$ are the conditioning variables (also called as context), and $\\theta$ are primary variables what get normalized. However, in the above equations, we don't see any conditioning variables. Can you modify the forward and backward equations such that they show how context is utilized?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BChsf4yt_59d"
      },
      "source": [
        "> **Task 2.3.** With all the equations to implement a conditional normalizing flow at hand, complete the AffineCouplingLayer class below..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN6mQfKz_59e"
      },
      "outputs": [],
      "source": [
        "class AffineCouplingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, context_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.split_idx = input_dim - (input_dim // 2) # first part gets more dims if input_dim is odd\n",
        "\n",
        "        # Define scale and shift networks\n",
        "        self.scale_net = nn.Sequential(\n",
        "            nn.Linear(self.split_idx + context_dim, 64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, input_dim - self.split_idx)\n",
        "        )\n",
        "        self.shift_net = nn.Sequential(\n",
        "            nn.Linear(self.split_idx + context_dim, 64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, input_dim - self.split_idx)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        # Split input tensor along the last dimension\n",
        "        x_identity = x[..., :self.split_idx]\n",
        "        x_transform = x[..., self.split_idx:]\n",
        "\n",
        "        # # Concatenate identity and context, pass them into the networks\n",
        "        # identity_context =\n",
        "        # scale =\n",
        "        # shift =\n",
        "\n",
        "        # Compute log-determinant of the Jacobian\n",
        "        # ldj =\n",
        "\n",
        "        # Affine transformation on x_transform\n",
        "        # z_transform =\n",
        "\n",
        "        # Concatenate unchanged part with transformed part\n",
        "        z = torch.cat((x_identity, z_transform), dim=-1)\n",
        "        return z, ldj\n",
        "\n",
        "    def inverse(self, z, context):\n",
        "        # Inverse transform: split z into identity and transformed parts\n",
        "        z_identity = z[..., :self.split_idx]\n",
        "        z_transform = z[..., self.split_idx:]\n",
        "\n",
        "        # # Concatenate identity and context, pass them into the networks\n",
        "        # identity_context =\n",
        "        # scale =\n",
        "        # shift =\n",
        "\n",
        "        # Compute log-determinant of the Jacobian\n",
        "        # ldj =\n",
        "\n",
        "        # Inverse affine transformation\n",
        "        # x_transform =\n",
        "\n",
        "        # Concatenate identity and transformed parts\n",
        "        x = torch.cat((z_identity, x_transform), dim=-1)\n",
        "        return x, ldj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O1L-MOn_59e"
      },
      "source": [
        "Subsequently, we can build a normalizing flow by stacking multiple coupling layers, and a prior distribution.\n",
        "\n",
        "One last missing piece is a permutation layer that ensures that each dimension is eventually used as the \"identity\" part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlMVkwqu_59f"
      },
      "outputs": [],
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "    \"\"\"\n",
        "    A normalizing flow model composed of a sequence of affine coupling layers and a prior distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, flows, prior=None):\n",
        "        super().__init__()\n",
        "        self.flows = nn.ModuleList(flows)\n",
        "        self.dim = self.flows[0].input_dim\n",
        "        # Initialize the prior distribution (device will be set correctly later)\n",
        "        if prior is None:\n",
        "            self.prior = torch.distributions.MultivariateNormal(\n",
        "                torch.zeros(self.dim), torch.eye(self.dim))\n",
        "        else:\n",
        "            self.prior = prior\n",
        "\n",
        "        self.train_loss = []\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        \"\"\"\n",
        "        Applies a sequence of flow transformations and accumulates the log-determinants.\n",
        "        \"\"\"\n",
        "        ldj = torch.zeros(x.shape[0], device=x.device)\n",
        "        for flow in self.flows:\n",
        "            x, ldj_ = flow(x, context)\n",
        "            ldj += ldj_\n",
        "        return x, ldj\n",
        "\n",
        "    def inverse(self, z, context):\n",
        "        \"\"\"\n",
        "        Inverts the flow transformation from latent space back to the input space.\n",
        "        \"\"\"\n",
        "        ldj = torch.zeros(z.shape[0], device=z.device)\n",
        "        for flow in reversed(self.flows):\n",
        "            z, ldj_ = flow.inverse(z, context)\n",
        "            ldj += ldj_  # log-determinants are already negated in inverse\n",
        "        return z, ldj\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, num_samples, context):\n",
        "        \"\"\"\n",
        "        Generate samples from the model given a context.\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        z = self.prior.sample((num_samples,)).to(device)\n",
        "        x, _ = self.inverse(z, context)\n",
        "        return x\n",
        "\n",
        "    def log_prob(self, x, context):\n",
        "        \"\"\"\n",
        "        Compute the log probability of x under the flow model.\n",
        "        \"\"\"\n",
        "        z, ldj = self(x, context)\n",
        "        log_pz = self.prior.log_prob(z)\n",
        "        return log_pz + ldj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCUT7rnZ_59f"
      },
      "outputs": [],
      "source": [
        "class PermutationLayer(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        # Create a random permutation for the feature indices.\n",
        "        perm = torch.randperm(num_features)\n",
        "        self.register_buffer(\"perm\", perm)\n",
        "        self.register_buffer(\"inv_perm\", torch.argsort(perm))\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        # Permuting the features; no effect on the log-determinant.\n",
        "        x_permuted = x[..., self.perm]\n",
        "        # Log-determinant is zero for a permutation\n",
        "        log_det = torch.zeros(x.size(0), device=x.device)\n",
        "        return x_permuted, log_det\n",
        "\n",
        "    def inverse(self, x, context):\n",
        "        # Inverse permutation\n",
        "        x_inv = x[..., self.inv_perm]\n",
        "        log_det = torch.zeros(x.size(0), device=x.device)\n",
        "        return x_inv, log_det\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGqIXxWx_59g"
      },
      "source": [
        "Now, let's put everything together and train a normalizing flow on the two moons model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUnqmt2T_59g"
      },
      "outputs": [],
      "source": [
        "# Handle dimensions\n",
        "input_dim = 2\n",
        "context_dim = 2\n",
        "n_layers = 4\n",
        "flows = []\n",
        "\n",
        "# Define the model and optimizer\n",
        "for i in range(n_layers):\n",
        "    flows.append(AffineCouplingLayer(input_dim, context_dim))\n",
        "    flows.append(PermutationLayer(input_dim))\n",
        "\n",
        "model = NormalizingFlow(flows)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzlgR1GW_59g"
      },
      "outputs": [],
      "source": [
        "# Fix an observed data point x_obs\n",
        "x_obs = torch.tensor([[0.0, 0.0]], dtype=torch.float32)\n",
        "\n",
        "# Create a grid of theta-values over which to evaluate the posterior\n",
        "batch_size = 100\n",
        "theta0_vals = torch.linspace(-2, 2, batch_size)\n",
        "theta1_vals = torch.linspace(-2, 2, batch_size)\n",
        "TH0, TH1 = torch.meshgrid(theta0_vals, theta1_vals, indexing='xy')\n",
        "theta_grid = torch.cat([TH0.reshape(-1,1), TH1.reshape(-1,1)], dim=1)\n",
        "\n",
        "# Training settings\n",
        "num_iter = 5000\n",
        "num_update_iter = 100\n",
        "batch_size = 100\n",
        "losses = []\n",
        "\n",
        "# Prepare figure\n",
        "fig, (ax_loss, ax_posterior) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "plt.ion()\n",
        "\n",
        "for i in range(num_iter):\n",
        "    # Generate data\n",
        "    theta = torch.rand((batch_size, 2), dtype=torch.float32) * 4 - 2\n",
        "    x = two_moons_sbi(theta)\n",
        "\n",
        "    # Standard training step\n",
        "    optimizer.zero_grad()\n",
        "    loss = -model.log_prob(x, theta).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Update plots interactively\n",
        "    if i % num_update_iter == 0:\n",
        "        ax_loss.cla()\n",
        "        ax_posterior.cla()\n",
        "\n",
        "        # Training loss\n",
        "        ax_loss.plot(losses, label='Train Loss')\n",
        "        ax_loss.set_title('Training Loss')\n",
        "        ax_loss.set_xlabel('Iteration')\n",
        "        ax_loss.set_ylabel('Negative Log-Likelihood')\n",
        "        ax_loss.legend()\n",
        "\n",
        "        # Approximate posterior\n",
        "        with torch.no_grad():\n",
        "            # Replicate x_obs for every point in theta_grid so the shape matches\n",
        "            x_obs_tiled = x_obs.repeat(theta_grid.shape[0], 1)\n",
        "\n",
        "            # Posterior ~ exp(log p(x_obs | theta))\n",
        "            post_vals = model.log_prob(x_obs_tiled, theta_grid).exp()\n",
        "            post_2d = post_vals.view(batch_size, batch_size)\n",
        "\n",
        "        # Contour-plot the posterior in theta-space\n",
        "        c = ax_posterior.contourf(\n",
        "            TH0.numpy(), TH1.numpy(), post_2d.numpy(),\n",
        "            levels=50, alpha=0.8\n",
        "        )\n",
        "        ax_posterior.set_title(f'Posterior at iteration {i}')\n",
        "        ax_posterior.set_xlabel(r'$\\theta_0$')\n",
        "        ax_posterior.set_ylabel(r'$\\theta_1$')\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(fig)\n",
        "\n",
        "plt.ioff();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NV42k7Z_59h"
      },
      "outputs": [],
      "source": [
        "# The really cool thing is that we don't need to re-run the algorithm\n",
        "# to infer posterior for a new data point! (more or less...)\n",
        "x_obs = torch.tensor([[0.1, 0.1]], dtype=torch.float32)\n",
        "\n",
        "# Create a grid of theta-values over which to evaluate the posterior\n",
        "batch_size = 100\n",
        "theta0_vals = torch.linspace(-3, 3, batch_size)\n",
        "theta1_vals = torch.linspace(-3, 3, batch_size)\n",
        "TH0, TH1 = torch.meshgrid(theta0_vals, theta1_vals, indexing='xy')\n",
        "theta_grid = torch.cat([TH0.reshape(-1,1), TH1.reshape(-1,1)], dim=1)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=[4, 4])\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Replicate x_obs for every point in theta_grid so the shape matches\n",
        "    x_obs_tiled = x_obs.repeat(theta_grid.shape[0], 1)\n",
        "\n",
        "    # Posterior ~ exp(log p(theta | x_obs))\n",
        "    post_vals = model.log_prob(x_obs_tiled, theta_grid).exp()\n",
        "    post_2d = post_vals.view(batch_size, batch_size)\n",
        "\n",
        "# Contour-plot the posterior in theta-space\n",
        "c = ax.contourf(\n",
        "    TH0.numpy(), TH1.numpy(), post_2d.numpy(),\n",
        "    levels=50, alpha=0.8\n",
        ")\n",
        "ax.set_title(f'Posterior at iteration {i}')\n",
        "ax.set_xlabel(r'$\\theta_0$')\n",
        "ax.set_ylabel(r'$\\theta_1$')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZEofcDo_59i"
      },
      "source": [
        "### **Repeat using `sbi` toolbox**\n",
        "\n",
        "Some semantics relevant to the `sbi` toolbox: it requires a `prior` and `simulator` to build two objects - an `inference` object that does the density estimation and a `posterior` from which you can sample and calculate log probabilities.\n",
        "\n",
        "> **Task 2.3.** Follow along the code below to see how we can use the `sbi` toolbox to train a normalizing flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdowq8-7_59i"
      },
      "outputs": [],
      "source": [
        "from sbi.analysis import pairplot\n",
        "from sbi.inference import NPE\n",
        "from sbi.utils import BoxUniform\n",
        "from sbi.utils.user_input_checks import (\n",
        "    check_sbi_inputs,\n",
        "    process_prior,\n",
        "    process_simulator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HbjDhsK_59j"
      },
      "outputs": [],
      "source": [
        "# Define prior\n",
        "num_dim = 2\n",
        "prior = BoxUniform(-3 * torch.ones(num_dim), 3 * torch.ones(num_dim))\n",
        "\n",
        "# Define simulator\n",
        "# We have already done this at the top (two_moons_sbi())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE8glgyO_59j"
      },
      "outputs": [],
      "source": [
        "# Check prior\n",
        "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
        "\n",
        "# Check simulator\n",
        "simulator = process_simulator(two_moons_sbi, prior, prior_returns_numpy)\n",
        "\n",
        "# Consistency check after making ready for sbi.\n",
        "check_sbi_inputs(simulator, prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IwlxGs7_59j"
      },
      "outputs": [],
      "source": [
        "# Generate samples from the prior\n",
        "num_simulations = 10000\n",
        "theta = prior.sample((num_simulations,))\n",
        "x = simulator(theta)\n",
        "print(\"theta.shape\", theta.shape)\n",
        "print(\"x.shape\", x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmtyJk7b_59k"
      },
      "outputs": [],
      "source": [
        "# Make an inference object and train it!\n",
        "inference = NPE(prior=prior)\n",
        "inference = inference.append_simulations(theta, x)\n",
        "density_estimator = inference.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9jpcbqh_59k"
      },
      "outputs": [],
      "source": [
        "# Build the posterior\n",
        "posterior = inference.build_posterior(density_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDSvN2Kr_59k"
      },
      "outputs": [],
      "source": [
        "samples = posterior.sample((10000,), x=[0, 0])\n",
        "_ = pairplot(\n",
        "    samples,\n",
        "    limits=[[-2, 2], [-2, 2]],\n",
        "    figsize=(6, 6),\n",
        "    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4FzKTlo_59l"
      },
      "source": [
        "### **Outro**\n",
        "We have successfully trained an affine coupling flow on the two moons model from scratch, and compared it to the `sbi` toolbox. Now, let's see how we can apply this to a real-world problem! Click below to open the next notebook.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/03_NFlows_BioMime.ipynb)\n",
        "\n",
        "#### Additional reading\n",
        "\n",
        "[1] [How do distributions transform under a change of variables?](https://theoryandpractice.org/stats-ds-book/distributions/change-of-variables.html), Kyle Cranmer  \n",
        "[2] [Density estimation using Real NVPs](https://arxiv.org/abs/1605.08803), Dinh et al. 2016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLRJegB8_59l"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}