{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roda10/Latent_Variable_Models/blob/main/notebooks/01_SBI_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2t_uVE2t_7K"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/01_SBI_Intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUkhVNJt_7N"
      },
      "source": [
        "## Intro to Simulation Based Inference (SBI)\n",
        "**Pranav Mamidanna, PhD** (p.mamidanna22@imperial.ac.uk), April 2025\n",
        "\n",
        "In many scientific and engineering problems, we have access to powerful **simulators** that can generate data $x$ that closely resemble real-world observations $x_{\\text{obs}}$. And our interest lies in the underlying parameters $\\theta$ that govern our problem of interest, often providing mechanistic or causal insight. Using **Bayesian inference**, we aim to estimate the probable values of the parameters that could have generated the observations $x_{\\text{obs}}$ we have acquired, i.e., $p(\\theta \\mid x_{\\text{obs}})$.\n",
        "\n",
        "However, if the simulator does not yield a closed-form likelihood $p(x \\mid \\theta)$, standard Bayesian tools (like MCMC) become challenging or impossible. This is known as the **likelihood-free** or **simulation-based** setting.\n",
        "\n",
        "In this notebook, we explore:\n",
        "1. How, even for a seemingly simple 2D “two-moons” simulator, deriving the exact likelihood is messy.  \n",
        "2. **Approximate Bayesian Computation (ABC)**, an approach that sidesteps the need for a closed-form likelihood by simulating data under proposed parameters and retaining those that generate data “close enough” to our observed dataset.  \n",
        "\n",
        "These ideas are a foundation for **Neural Simulation-Based Inference**, which trains neural networks to approximate posteriors or likelihoods when simulations are available but explicit probability formulas are not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uaOHBjMt_7P"
      },
      "outputs": [],
      "source": [
        "# Uncomment the line below to ensure that your colab env has corner installed\n",
        "!python -m pip install corner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB16ygzrt_7R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, interact_manual, FloatSlider, IntSlider\n",
        "import corner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVa52Lv3t_7S"
      },
      "source": [
        "### **Two Moons** _in the Posterior_\n",
        "\n",
        "Consider the following problem - we have a simulator $\\mathbf{Sim} : \\theta \\to x$, which computes $x$ as follows:\n",
        "\n",
        "$$a \\sim Unif\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)$$\n",
        "\n",
        "$$r \\sim Normal(0.1,0.01^2)$$\n",
        "\n",
        "$$p = (r \\cos(a) + 0.5, r \\sin(a))$$\n",
        "\n",
        "$$x^\\top = p + \\left( \\frac{-\\left| \\theta_1 + \\theta_2 \\right|}{\\sqrt{2}}, \\frac{-\\theta_1 + \\theta_2}{\\sqrt{2}} \\right)$$\n",
        "\n",
        "Which in code looks like -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LGHfIS2t_7T"
      },
      "outputs": [],
      "source": [
        "def two_moons_sbi(theta, n_samples=100, sigma=0.01):\n",
        "    \"\"\"Generate a two moons posterior\"\"\"\n",
        "    alpha = np.random.uniform(-np.pi/2, np.pi/2, n_samples)\n",
        "    r = sigma * np.random.randn(n_samples) + 1\n",
        "    x_1 = r * np.cos(alpha) + 0.5 - np.abs(theta[0] + theta[1])/np.sqrt(2)\n",
        "    x_2 = r * np.sin(alpha) + (- theta[0] + theta[1])/np.sqrt(2)\n",
        "\n",
        "    x =  np.stack([x_1, x_2], axis=-1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc9C1VuCt_7U"
      },
      "source": [
        "And generates data that looks like below. Play around with the simulator below to get a sense of how the data looks like for different values of $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxG1xXk5t_7U"
      },
      "outputs": [],
      "source": [
        "def plot_data(theta1=0.5, theta2=-0.5):\n",
        "    theta = np.array([theta1, theta2])\n",
        "    x = two_moons_sbi(theta, n_samples=200)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.scatter(x[:, 0], x[:, 1], s=5)\n",
        "    plt.xlim(-3, 3); plt.ylim(-3, 3)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Create interactive sliders\n",
        "theta1_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta1')\n",
        "theta2_slider = FloatSlider(value=-0.5, min=-2.0, max=2.0, step=0.1, description='theta2')\n",
        "\n",
        "interact(plot_data, theta1=theta1_slider, theta2=theta2_slider);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4jqwnDYt_7W"
      },
      "source": [
        "> **Task 1.1** Where are the two moons?  \n",
        "(Hint: how does the data look like when you simulate with $\\theta$ is $(\\theta_1, \\theta_2)$, and $(-\\theta_2, -\\theta_1)$ ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl6e9TQBt_7X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[4, 4])\n",
        "# UNCOMMENT AND FILL IN THE CODE BELOW!\n",
        "# plt.scatter(*two_moons_sbi(\"Set theta here\").T)\n",
        "# plt.scatter(*two_moons_sbi(\"Set theta here\").T)\n",
        "\n",
        "plt.xlim(-3, 3); plt.ylim(-3, 3)\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwFDszDt_7X"
      },
      "source": [
        "Play around with the simulator below to get a sense of the posterior distribution for a single observation here shown as a red star. By playing around with the theta you should (hopefully) uncover the two moons in the posterior!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCfgqCL-t_7Y"
      },
      "outputs": [],
      "source": [
        "# We'll define a global list to hold our \"accepted\" parameter values\n",
        "accepted_params = []\n",
        "\n",
        "def update_plots(theta1=0.0, theta2=0.0):\n",
        "    \"\"\"\n",
        "    For the given (theta1, theta2):\n",
        "    1) Generate data and plot it in the data space.\n",
        "    2) Check if x_obs is within threshold distance of any point in that data.\n",
        "       If yes, record (theta1, theta2) in a global list.\n",
        "    3) Plot the \"accepted\" parameter list so far in the parameter space.\n",
        "    \"\"\"\n",
        "    global accepted_params\n",
        "\n",
        "    x_obs = np.array([0.0, 0.0])\n",
        "    threshold = 0.2   # acceptance threshold for \"close enough\"\n",
        "    col = 'C0' # Let's change color of samples when there is a match!\n",
        "\n",
        "    # Generate data from user specified theta\n",
        "    theta = np.array([theta1, theta2])\n",
        "    x_sim = two_moons_sbi(theta, n_samples=20)\n",
        "\n",
        "    # Check if any of the points in x_sim lie close enough to x_obs\n",
        "    # We'll measure Euclidean distance from each simulated point to x_obs\n",
        "    distances = np.linalg.norm(x_sim - x_obs, axis=1)\n",
        "    if np.any(distances < threshold):\n",
        "        accepted_params.append(theta)\n",
        "        accepted_params.append(np.array([-theta2, -theta1]))\n",
        "        col='C1'\n",
        "    else:\n",
        "        col='C0'\n",
        "\n",
        "    # Plot the results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "    # Data space\n",
        "    axes[0].scatter(*x_sim.T, color=col, alpha=0.6, label=\"Simulated data\")\n",
        "\n",
        "    # Parameter space\n",
        "    axes[1].plot(theta1, theta2, 'rx', markersize=8, label=r\"Current $\\theta$\")\n",
        "    if len(accepted_params) > 0:\n",
        "        axes[1].scatter(*np.array(accepted_params).T, alpha=0.3, label=r\"$x_{obs} \\mid \\theta$\")\n",
        "\n",
        "    # Prettify\n",
        "    axes[0].set_title('Data space')\n",
        "    axes[0].plot(0, 0, 'r*', markersize=10, label=r\"$x_{obs} = [0,0]$\")\n",
        "    axes[0].set_xlim(-3, 3); axes[0].set_ylim(-3, 3); axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "    axes[1].set_title(\"Parameter Space\")\n",
        "    axes[1].set_xlabel(r\"$\\theta_1$\"); axes[1].set_ylabel(r\"$\\theta_2$\")\n",
        "    axes[1].set_xlim(-2.0, 2.0); axes[1].set_ylim(-2.0, 2.0)\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create sliders for theta1 and theta2\n",
        "theta1_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta_1')\n",
        "theta2_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta_2')\n",
        "\n",
        "interact(update_plots, theta1=theta1_slider, theta2=theta2_slider);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SwuBcCdt_7Z"
      },
      "source": [
        "Oh, btw, you basically did simulation based inference :)\n",
        "\n",
        "But let's take a step back. Given we have an observation $x_{\\text{obs}}$ from this data generating process, how can we infer the parameters $\\theta$ that could have generated it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6o2qI03t_7Z"
      },
      "source": [
        "**Bonus: Can we get an exact likelihood?**\n",
        "\n",
        "Even in this simple “two-moons” simulator, the closed-form likelihood $p(x\\mid\\theta)$ is nontrivial to write down. Here’s the basic idea - our simulator draws:\n",
        "$$\n",
        "\\alpha \\sim \\mathbf{U}\\bigl(-\\tfrac{\\pi}{2}, \\tfrac{\\pi}{2}\\bigr),\n",
        "\\quad\n",
        "r \\sim \\mathbf{N}(0, \\sigma^2),\n",
        "$$\n",
        "and maps $(\\alpha, r)$ to a data point $x$ via:\n",
        "$$\n",
        "x_1 = r \\cos(\\alpha) + 0.25 - \\dfrac{|\\theta_1 + \\theta_2|}{\\sqrt{2}},\n",
        "\\quad\n",
        "x_2 = r \\sin(\\alpha) + \\dfrac{\\theta_2 - \\theta_1}{\\sqrt{2}}.\n",
        "$$\n",
        "\n",
        "This means that we can use the **change of variables formula** to go from $(\\alpha, r) \\to x$. That is, we can write the density of $x$ as\n",
        "$$\n",
        "p(x \\mid \\theta) \\propto ~ p(\\alpha) ~ p(r) ~ \\left| \\dfrac{d(\\alpha, r)}{d(x_1, x_2)} \\right|\n",
        "$$\n",
        "\n",
        "> **Optional Task 1.2:** derive a simplified expression for the likelihood.\n",
        "\n",
        "In larger or more realistic simulators (with more dimensions, complicated physics, etc.), deriving such an expression by hand may be **extremely** difficult or effectively impossible. **This** is why we turn to likelihood-free methods—like **Approximate Bayesian Computation** or **Neural Simulation-Based Inference**—which avoid having to evaluate $p(x\\mid\\theta)$ directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnAG9F1vt_7a"
      },
      "source": [
        "### **Approximate Bayesian Computation (ABC)**\n",
        "\n",
        "Approximate Bayesian Computation (ABC) is a class of inference methods designed for settings where the likelihood function $p(x \\mid \\theta)$ is *intractable*—but we can still **simulate** from it. The basic idea is this -- simulators let us verify _whether_ a particular $\\theta$ could have generated an observed $x_{obs}$! If we simulate _A LOT_ of $\\theta$'s, then we can use the sampled $x$'s to _infer_ the posterior distribution!\n",
        "\n",
        "**Ingredients list**\n",
        "\n",
        "- A prior distribution $p(\\theta)$: We can sample from this to generate candidate parameter values.\n",
        "- A simulator: Given $\\theta$, we simulate synthetic data $x_{\\text{sim}} \\sim p(x \\mid \\theta)$.\n",
        "- A distance function $d(x_{\\text{sim}}, x_{\\text{obs}})$: Measures how close the simulated data is to the observed data.\n",
        "- A threshold $\\varepsilon$: Determines how \"close\" is close enough.\n",
        "\n",
        "**The Basic ABC Algorithm**\n",
        "\n",
        "Repeat until you have $N$ accepted samples:\n",
        "1. Sample $\\theta^* \\sim p(\\theta)$ from the prior.\n",
        "2. Simulate $x_{\\text{sim}} \\sim p(x \\mid \\theta^*)$ using the simulator.\n",
        "3. If $d(x_{\\text{sim}}, x_{\\text{obs}}) < \\varepsilon$, accept $\\theta^*$ as a posterior sample.\n",
        "\n",
        "The collection of accepted $\\theta$ values approximates the posterior $p(\\theta \\mid x_{\\text{obs}})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw8pspQqt_7a"
      },
      "source": [
        "> **Task 1.3** Based on the algorithm above, complete the following code to run your own ABC inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260Qp22Yt_7a"
      },
      "outputs": [],
      "source": [
        "def abc(x_obs, eps=0.1, n_samples=1000):\n",
        "    \"\"\"ABC algorithm for two moons model.\n",
        "\n",
        "    Args:\n",
        "        x_obs (np.ndarray): Observation from the two moons model.\n",
        "        eps (float, optional): Acceptance threshold. Defaults to 0.1.\n",
        "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Samples obtained for approximating the posterior p(theta|x_obs).\n",
        "        np.ndarray: Acceptance flag for each sample.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    accept_flag = []\n",
        "    n_accepted = sum(accept_flag)\n",
        "    n_sim = 100 # samples to simulate each time\n",
        "\n",
        "    # Make a progress bar to see how quickly we accept\n",
        "    pbar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n",
        "\n",
        "    while sum(accept_flag) < n_samples:\n",
        "        ## TO-DO: Write the code to sample a parameter from the prior,\n",
        "        # simulate data, and compute the distance to the observed data\n",
        "\n",
        "        # Bookkeeping\n",
        "        samples.append(theta_candidate)\n",
        "\n",
        "        if np.any(dist < eps):\n",
        "            accept_flag.append(1) # bookkeeping accepted thetas\n",
        "            n_accepted += np.sum(dist < eps)\n",
        "            acceptance_ratio = n_accepted / (len(samples) * n_sim)\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"acceptance_ratio={acceptance_ratio:.3f}\")\n",
        "        else:\n",
        "            accept_flag.append(0)\n",
        "\n",
        "    pbar.close()\n",
        "    return np.array(samples), np.array(accept_flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsspM3BAt_7b"
      },
      "outputs": [],
      "source": [
        "# Verify this runs without any errors!\n",
        "candidates, flags = abc([0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFoi7Kait_7c"
      },
      "source": [
        "Let's see ABC in action! Can it compete with how you explored the parameter space previously?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVgKRfSnt_7c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "global_accepted_samples = []\n",
        "\n",
        "def run_abc(n_samples=100, eps=0.1):\n",
        "    \"\"\"\n",
        "    Each time run_interact is pressed:\n",
        "    1. Run ABC to get n_samples new samples.\n",
        "    2. Display them one by one on the plot, maintaining history of all previous runs.\n",
        "    \"\"\"\n",
        "    global global_accepted_samples\n",
        "\n",
        "    # Run ABC with a single observed point [0,0]\n",
        "    samples, accept_flag = abc(np.array([0.0, 0.0]), eps, n_samples)\n",
        "\n",
        "    # Show new samples one-by-one\n",
        "    for i, sample in enumerate(samples):\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
        "\n",
        "        # First plot all previously accepted samples\n",
        "        if global_accepted_samples:\n",
        "            ax.scatter(*np.array(global_accepted_samples).T, alpha=0.6, label='Previous samples')\n",
        "\n",
        "        # Plot current sample\n",
        "        ax.plot(sample[0], sample[1], 'rx', markersize=10, label='Current sample')\n",
        "\n",
        "        # If accepted, add to plot and global list\n",
        "        if accept_flag[i] == 1:\n",
        "            global_accepted_samples.append(sample)\n",
        "            ax.scatter(sample[0], sample[1], alpha=0.6)\n",
        "\n",
        "        # Prettify\n",
        "        ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)\n",
        "        ax.set_xlabel(r'$\\theta_1$'); ax.set_ylabel(r'$\\theta_2$')\n",
        "        ax.set_title(f\"ABC Posterior Samples\")\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.show()\n",
        "        # plt.pause(0.005)  # Small pause for visualization\n",
        "\n",
        "samples_slider = IntSlider(value=50, min=10, max=200, step=10, description='Number of samples to acquire')\n",
        "eps_slider = FloatSlider(value=0.2, min=0.1, max=1, step=0.1, description='Acceptance threshold')\n",
        "\n",
        "interact_manual(run_abc, n_samples=samples_slider, eps=eps_slider);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FYxtXyst_7d"
      },
      "outputs": [],
      "source": [
        "# Plot the posterior distribution using corner\n",
        "samples, accept_flag = abc([0, 0])\n",
        "figure = corner.corner(samples[accept_flag==1], labels=['theta1', 'theta2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP11UobEt_7d"
      },
      "source": [
        "> **Task 1.4 Discuss with your neighbour**\n",
        "1. How does the inference (the approximate posterior distribution) depend on the number of samples in ABC?\n",
        "2. How does changing epsilon threshold affect the posterior?\n",
        "3. Here, we have performed inference for a single observation. What happens when we have multiple $x_{obs}$? Do you think ABC scales well?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ggogle.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "3IG16U3MuMGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ttomfWt_7d"
      },
      "source": [
        "### Outro\n",
        "\n",
        "While ABC is very simple and elegant, it can be quite inefficient in high dimensions, or when choosing the right distance metric or threshold is hard. These limitations motivate more scalable approaches like **Neural Simulation-Based Inference (Neural SBI)**, which we will cover next.\n",
        "\n",
        "Click below to open the notebook in a new window.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/02_NFlows.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}